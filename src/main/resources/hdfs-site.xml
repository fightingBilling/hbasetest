<?xml version="1.0"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
<property>
  <name>dfs.permissions.superusergroup</name>
  <value>hdfs</value>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>/hadoop_data/data/dfs/nn</value>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>/hadoop_data/data01/dfs/dn,/hadoop_data/data02/dfs/dn,/hadoop_data/data03/dfs/dn,/hadoop_data/data04/dfs/dn,/hadoop_data/data05/dfs/dn,/hadoop_data/data06/dfs/dn,/hadoop_data/data07/dfs/dn,/hadoop_data/data08/dfs/dn,/hadoop_data/data09/dfs/dn,/hadoop_data/data10/dfs/dn,/hadoop_data/data11/dfs/dn,/hadoop_data/data12/dfs/dn,/hadoop_data/data13/dfs/dn,/hadoop_data/data14/dfs/dn,/hadoop_data/data15/dfs/dn,/hadoop_data/data16/dfs/dn,/hadoop_data/data17/dfs/dn,/hadoop_data/data18/dfs/dn,/hadoop_data/data19/dfs/dn,/hadoop_data/data20/dfs/dn,/hadoop_data/data21/dfs/dn,/hadoop_data/data22/dfs/dn</value>
</property>

<property>
  <name>dfs.nameservices</name>
  <value>mycluster</value>
</property>

<property>
  <name>dfs.ha.namenodes.mycluster</name>
  <value>nn1,nn2</value>
</property>

<property>
  <name>dfs.namenode.rpc-address.mycluster.nn1</name>
  <value>dev01.dc.ztgame.com:8020</value>
</property>

<property>
  <name>dfs.namenode.rpc-address.mycluster.nn2</name>
  <value>dev03.dc.ztgame.com:8020</value>
</property>

<property>
  <name>dfs.namenode.http-address.mycluster.nn1</name>
  <value>dev01.dc.ztgame.com:50070</value>
</property>

<property>
  <name>dfs.namenode.http-address.mycluster.nn2</name>
  <value>dev03.dc.ztgame.com:50070</value>
</property>

<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://dev01.dc.ztgame.com:8485;dev03.dc.ztgame.com:8485;dev04.dc.ztgame.com:8485/mycluster</value>
</property>

<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/hadoop_data/data/dfs/jn</value>
</property>

<property>
  <name>dfs.client.failover.proxy.provider.mycluster</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>

<property>
  <name>dfs.ha.fencing.methods</name>
  <value>sshfence</value>
</property>

<property>
  <name>dfs.ha.fencing.ssh.private-key-files</name>
  <value>/var/lib/hadoop-hdfs/.ssh/id_dsa</value>
</property>

<property>
  <name>dfs.ha.automatic-failover.enabled</name>
  <value>true</value>
</property>

<!-- Use WebHDFS -->
<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
</property>

<!-- General HDFS security config -->
<property>
  <name>dfs.block.access.token.enable</name>
  <value>true</value>
</property>

<!-- NameNode security config -->
<property>
  <name>dfs.namenode.keytab.file</name>
  <value>/etc/hadoop/conf/hdfs.keytab</value> 
</property>

<property>
  <name>dfs.namenode.kerberos.principal</name>
  <value>hdfs/_HOST@ZTGAME.COM</value>
</property>

<property>
  <name>dfs.namenode.kerberos.internal.spnego.principal</name>
  <value>HTTP/_HOST@ZTGAME.COM</value>
</property>

<!-- DataNode security config -->
<property>
  <name>dfs.datanode.data.dir.perm</name>
  <value>700</value>
</property>

<property>
  <name>dfs.datanode.address</name>
  <value>0.0.0.0:50010</value>
</property>

<property>
  <name>dfs.datanode.http.address</name>
  <value>0.0.0.0:50075</value>
</property>

<property>
  <name>dfs.datanode.keytab.file</name>
  <value>/etc/hadoop/conf/hdfs.keytab</value>
</property>

<property>
  <name>dfs.datanode.kerberos.principal</name>
  <value>hdfs/_HOST@ZTGAME.COM</value>
</property>

<!--Quorum-based Storage security config -->
<property>
  <name>dfs.journalnode.keytab.file</name>
  <value>/etc/hadoop/conf/hdfs.keytab</value>
</property>

<property>
  <name>dfs.journalnode.kerberos.principal</name>
  <value>hdfs/_HOST@ZTGAME.COM</value>
</property>

<property>
  <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
  <value>HTTP/_HOST@ZTGAME.COM</value>
</property>

<!--WebHDFS security config -->
<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
</property>

<property>
  <name>dfs.web.authentication.kerberos.principal</name>
  <value>HTTP/_HOST@ZTGAME.COM</value>
</property>

<property>
  <name>dfs.web.authentication.kerberos.keytab</name>
  <value>/etc/hadoop/conf/hdfs.keytab</value> 
</property>

<property>
  <name>hadoop.http.authentication.type</name>
  <value>kerberos</value>
</property>

<!-- newadd_mxp -->
<property>
  <name>dfs.namenode.logging.level</name>
  <value>info</value>
  <description>The logging level for dfs namenode. Other values are "dir"(trac
e namespace mutations), "block"(trace block under/over replications and block
creations/deletions), or "all".</description>
</property>

<property>
  <name>dfs.datanode.handler.count</name>
  <value>20</value>
  <description>The number of server threads for the datanode.</description>
</property>

<property>
  <name>dfs.datanode.du.reserved</name>
  <value>32212254720</value>
  <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.</description>
</property>

<property>
  <name>dfs.blocksize</name>
  <value>134217728</value>
  <description>The default block size for new files.</description>
</property>

<property>
  <name>dfs.namenode.handler.count</name>
  <value>20</value>
  <description>The number of server threads for the namenode.</description>
</property>

<property>
  <name>dfs.namenode.decommission.interval</name>
  <value>30</value>
  <description>Namenode periodicity in seconds to check if decommission is complete.</description>
</property>

<property>
  <name>dfs.namenode.decommission.nodes.per.interval</name>
  <value>5</value>
  <description>The number of nodes namenode checks if decommission is complete
in each dfs.namenode.decommission.interval.</description>
</property>

<property>
  <name>dfs.namenode.replication.interval</name>
  <value>3</value>
  <description>The periodicity in seconds with which the namenode computes repliaction work for datanodes. </description>
</property>

<property>
  <name>dfs.namenode.accesstime.precision</name>
  <value>3600000</value>
  <description>The access time for HDFS file is precise upto this value. 
   The default value is 1 hour. Setting a value of 0 disables  access times for HDFS. </description>
</property>

<property>
  <name>dfs.support.append</name>
  <value>true</value>
  <description>Does HDFS allow appends to files?</description>
</property>

<property>
  <name>dfs.datanode.failed.volumes.tolerated</name>
  <value>3</value>
  <description>The number of volumes that are allowed to
fail before a datanode stops offering service. By default
any volume failure will cause a datanode to shutdown.</description>
</property>

<!-- Improve Performance for Local Reads -->
<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
</property>

<property>
  <name>dfs.client.read.shortcircuit.streams.cache.size</name>
  <value>1000</value>
</property>

<property>
  <name>dfs.client.read.shortcircuit.streams.cache.size.expiry.ms</name>
  <value>1000</value>
</property>

<property>
  <name>dfs.domain.socket.path</name>
  <value>/var/run/hadoop-hdfs/dn._PORT</value>
</property>

<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>8192</value>
</property>

<property>
  <name>dfs.client.file-block-storage-locations.timeout.millis</name>
  <value>30000</value>
</property>

<property>
  <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
  <value>true</value>
</property>

<property>
  <name>dfs.nfs3.dump.dir</name>
  <value>/tmp/.hdfs-nfs</value>
</property>

<property>
   <name>hadoop.proxyuser.hdfs.groups</name>
   <value>*</value>
   <description>
   Set this to '*' to allow the gateway user to proxy any group.
   </description>
</property>
<property>
   <name>hadoop.proxyuser.hdfs.hosts</name>
   <value>*</value>
   <description>
    Set this to '*' to allow requests from any hosts to be proxied.
   </description>
</property>

</configuration>
